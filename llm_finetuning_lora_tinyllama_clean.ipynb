{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ehaBJF-5H98",
    "outputId": "5873fb35-d4d5-4929-f687-ac254f4574bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip -q uninstall -y bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173,
     "referenced_widgets": [
      "7729338536e94ee6b47923122670f6af",
      "6abbdd212e524c5c8ea4801e3c8654fc",
      "7df8c9c20915444bab598c2c32c93445",
      "ac50c5765f3e4faaa6e74c4c0bcef6ac",
      "001cb4662ae84cc0baa2760ce7cddcd7",
      "bd80b5d6d08e42e3bd7e30f871833663",
      "996e5ad562b94564942b77c75489429f",
      "0082b5266e9b4bfda8fa1871b0bc5a7d",
      "f9d9dcd80487425ab943012b16a5b1b3",
      "3029198f236340c2b97c54c1d4ff6643",
      "7513252a640a4d9daacba42cc922c99e"
     ]
    },
    "id": "2luE6_p88tJc",
    "outputId": "cb2436a9-4ade-497e-cab2-37f4db8cf9c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 760 Eval: 40\n",
      "### Instruction:\n",
      "Explain LoRA (Low-Rank Adaptation) in 3 clear paragraphs.\n",
      "\n",
      "### Response:\n",
      "One of the main strengths of LoRA (Low-Rank Adaptation) is its ability to enhance generalization. By applying this concept correctly, models become more robust and better suited for real-world deployment.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_file = \"/content/autotrain_text.jsonl\"\n",
    "ds = load_dataset(\"json\", data_files=data_file)[\"train\"]\n",
    "ds = ds.train_test_split(test_size=0.05, seed=42)\n",
    "train_ds = ds[\"train\"]\n",
    "eval_ds  = ds[\"test\"]\n",
    "\n",
    "print(\"Train:\", len(train_ds), \"Eval:\", len(eval_ds))\n",
    "print(train_ds[0][\"text\"][:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400,
     "referenced_widgets": [
      "fa3bcd3ca45546828ae0929ba252d5bd",
      "11dedda2e1404cdbb9ba9b68336765c8",
      "66b741a0b40944aaa54461d42c45ca62",
      "52331a0fcd764c4689510908326b3ca8",
      "08cf762de658475ca970c2a0a26d6a0d",
      "c8d89f72a526485eb0b2872a25e13d53",
      "9056e6459eb246c190bf0f1512c8429f",
      "72cce5f7f4dc4b7bbdf710fda4502501",
      "0860e4ac088b47af9f27225ca7ab5b6f",
      "0dd95f3f811a40f2a5dd1cd825cdc16c",
      "a32af5a1a2a449a5bb95032387db7ab6",
      "8a84d8239b654d4092c338b7137ce7b4",
      "2b9df7a5c3e4434cb4577cea117c758c",
      "c4de779051dd4144b2655d30b9eceb74",
      "3c7ff30e09934c9d93e7dee27fde7bb9",
      "a205a94431594943ab0d352911929172",
      "c269af96a3a94ea8bceceef50c891d97",
      "c8edc360a4874489bb166a4f28603fda",
      "dc0357e3e208410497819678204c97d3",
      "11f1f7aefe50443d9590e92525c825a6",
      "a68eb22d6eef4d2a8836236d4ec77f1c",
      "8de2a9e8263d4bc1966f96184bb4da4e",
      "759c9333925b4678ad6ef3914e51e2ae",
      "49ea9542b0de4413980aa520e33687ef",
      "c785043e00de48cf8a572056929547dd",
      "2a6877e706824364b4ac8b19846bc88b",
      "a378095736234dfab6d424157dfaa3d7",
      "6cba22c5e1624bfbb826b02bd46dac24",
      "2186c7bdc6854ede8955642916a4510c",
      "29ce137405db4ee08b11bd7d46f5f059",
      "9f2d873668fb4dbdb04517e70cb54db5",
      "576d9babf2954432a71ba2ab12501b0b",
      "09f699d9b7fc48bda26a07ace8d34a4f",
      "af04ad1309a54bf481f7d9340cdd107b",
      "5c44f2f6a14046afa5c7e21170b59b53",
      "77ca4eb4f4f740679f4767f90293b5a5",
      "a3968761756340e1b3c1f420b6b808c6",
      "0f79f388228545babd62bc62acdec9b1",
      "3a72bfda8efd4611a0882140ec4f3852",
      "6263d3acc26546ed963a62d09fba7814",
      "058f1abfdb6f4c3e8e8e0889376b7c93",
      "4c03912a8b0c40fb88136f0502e427c7",
      "2159313541a347fa981cdd8a30af8421",
      "c515737aab0d4096bf3876ec5cea9b1e",
      "8c0f0df9959c4d2392ade570ed5c8a30",
      "64a51e007cd4431ea9fa6f916fcc1e93",
      "7b22615fe4854252a774ef1260c1e469",
      "abb9de86ecd64e52b6090a72c7945c07",
      "f145710a8ae6429ebf1dad79fff7cae8",
      "2430f918971f494a9588564ef6623b9a",
      "c12caed848554696a5febefe5cb2be02",
      "057f11c462f34cac97cf33ffd168bb3f",
      "1c9a7a63686e4d6b839fd9ce2f1325bc",
      "84ca02483a7e432f96307f1848c5ba07",
      "c7637ead0b8041db92b6bee7157a6b9a",
      "e8393487adc94d9bbab797ca617afb06",
      "18a3d2a8d51a4fd59b7d8de629de7c72",
      "bf4dfa259d6944bf993e77b49dc90e35",
      "4407fd85a8fb45388e4389e7817a7a29",
      "34bee734bea34d9180d2261af667a4fa",
      "f978670ed7ae4e41a2f1d8d79c79b173",
      "8e342b260b284ef6bfd8d99c5854e216",
      "f1cd2f03dc1e42f59d92dc3d2c4140e9",
      "197741d3594042439c835cac48f14364",
      "a253c50c204e49cd8ce643eee3336109",
      "62a1cb50dc22429297922db8655085ac",
      "3931477d915f4f358b8ee0381f4fd8b8",
      "4fcefcae17cc47f0b033f425c7d1a3ff",
      "0d7c7d0f30e94cb1b0294c99c608e123",
      "63ba66fb92144f9ab7f0188c6a6c6585",
      "9bcaad72b400403b9cf6a9c8dda03954",
      "7994f94a93c44c5592ff7bb7cbe7ce2e",
      "3559bea82430474595bb7b714452e15b",
      "152c6ac574a74691b0a0282efbb28eb6",
      "4cb740cc5d0c452faca78c668992ef57",
      "80ce347beb0c441988aa5b59e297f7c9",
      "11d91b7041ee4aad9738881e0948bcc3"
     ]
    },
    "id": "UI9Pp61I8ukE",
    "outputId": "29bf12b1-40ee-41e5-af66-55f1831ae20b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.config.use_cache = False\n",
    "print(\"‚úÖ Model loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98,
     "referenced_widgets": [
      "987e606ea92240ad85b1022b725cf913",
      "aea17d57e31f47cc9b00b093b51a85b2",
      "a17fc08b3b0649549c2f05330fe9e121",
      "d0de60d9b045433a835fcea3f80c2f1b",
      "36b4cc0d4c9845cca2aef279b7900d99",
      "6f3628431981482db569d053603a1fae",
      "5eb773d28dfb4c0e9c604cdafe036889",
      "23be4f12dddd410b856aa65aa62cb5de",
      "42de214583c640fd96ce1c0e107b599d",
      "023105d78595419481e84301a8c63ff1",
      "9e5b76894c60440fab2e76d550b24685",
      "7be39189b8f045a7a0754d888678eec5",
      "fad892a8edff472fbd0046fb74efdacd",
      "56153591e9ce4e09bd038fc5f80b2970",
      "77064afd64114e2883bbb656257478a8",
      "5692d6a5d23a4853a3e168abd9b9d431",
      "e929f1b4d0064a3aa4dbaf89b9330798",
      "c5982d2015c84250b1aac8797f13bb04",
      "1a81944693764719ad00190a2e7a3061",
      "56e580bbc8bc45e2b2184a70f395fc49",
      "8421f9ff1e304ace8659a0d7730b91e8",
      "e0443587f85147b1b2323399ae95366f"
     ]
    },
    "id": "QJgp6SYo8uhc",
    "outputId": "b73c2dae-1156-4e2d-e722-aa04e4960951"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenization done\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "max_length = 512\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "train_tok = train_ds.map(tokenize_fn, batched=True, remove_columns=train_ds.column_names)\n",
    "eval_tok  = eval_ds.map(tokenize_fn,  batched=True, remove_columns=eval_ds.column_names)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "print(\"‚úÖ Tokenization done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dimfkggm8ueE",
    "outputId": "e19c7d8c-2478-422c-b43e-6b8c4c1b2b2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,505,600 || all params: 1,104,553,984 || trainable%: 0.4079\n",
      "‚úÖ LoRA attached\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "print(\"‚úÖ LoRA attached\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "3anWf0CO8ubj",
    "outputId": "4f0dbbd6-e7b9-49c7-e9fd-a9965ea4b1fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='285' max='285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [285/285 06:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.297000</td>\n",
       "      <td>0.236547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.144700</td>\n",
       "      <td>0.146174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.134700</td>\n",
       "      <td>0.135155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.125600</td>\n",
       "      <td>0.129473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.128100</td>\n",
       "      <td>0.127168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training finished\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "out_dir = \"/content/tinyllama_lora_out\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=out_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "\n",
    "    eval_strategy=\"steps\",   # ‚úÖ ÿ®ÿØŸÑ evaluation_strategy\n",
    "    eval_steps=50,\n",
    "\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=eval_tok,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"‚úÖ Training finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xoWtP3T8uY1",
    "outputId": "b271fc27-c82a-47b6-8054-1d8067688880"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved adapter to: /content/tinyllama_lora_adapter\n"
     ]
    }
   ],
   "source": [
    "adapter_dir = \"/content/tinyllama_lora_adapter\"\n",
    "model.save_pretrained(adapter_dir)\n",
    "tokenizer.save_pretrained(adapter_dir)\n",
    "print(\"‚úÖ Saved adapter to:\", adapter_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RmE9grZD857L",
    "outputId": "62984eca-510d-4cef-dbde-a561295b9007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "What is synthetic data?\n",
      "\n",
      "### Response:\n",
      "Synthetic data is a fundamental concept in artificial intelligence that plays a critical role in improving the performance and reliability of modern models. It helps systems understand complex patterns and generate meaningful outputs across various tasks. Without synthetic data, many modern applications would struggle to achieve high accuracy and reliability.\n",
      "\n",
      "From a theoretical perspective, synthetic data represents a key advancement in the field of artificial intelligence. Researchers continue to explore its potential to improve scalability, interpretability, and efficiency.\n",
      "\n",
      "In practice, synthetic data helps machines understand complex patterns and generate meaningful outputs across\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "ft = PeftModel.from_pretrained(base, adapter_dir)\n",
    "ft.eval()\n",
    "\n",
    "prompt = \"### Instruction:\\nWhat is synthetic data?\\n\\n### Response:\\n\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(ft.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = ft.generate(**inputs, max_new_tokens=120, do_sample=True, temperature=0.7, top_p=0.9)\n",
    "\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9z5Ot37B-lY_",
    "outputId": "e49d1cb3-35d9-47b7-f4f5-ccf62ad8fae9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "What is synthetic data?\n",
      "\n",
      "### Response:\n",
      "In practical terms, synthetic data allows AI systems to operate more efficiently and accurately. It is widely applied in areas such as natural language processing, recommendation systems, and intelligent assistants. Without synthetic data, many modern apps would be difficult or impossible to understand.\n",
      "\n",
      "A real-world example of synthetic data can be seen in applications like ChatGPT, autonomous systems, and advanced data analysis tools, where intelligent behavior depends heavily on this concept. To become a successful engineer, students and professionals should embrace this fundamental theory of information processing.\n",
      "\n",
      "From a academic perspective, synthetic data represents a key advancement in the field of artificial intelligence. Researchers continue to explore its potential to improve scalability, interpretability, and efficiency. Future developments may also focus on generating genuine intelligent behaviors across large language models.\n",
      "\n",
      "In a more broad sense, synthetic data represents a key connection between theoretical foundations and practical implementations in modern systems. It helps researchers and developers understand complex problems better and improve their work over time.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = ft.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=250,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pfkpnQ1ANPpw",
    "outputId": "36629e65-a6f6-43a7-df24-515d0ea3cce8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 800 high-quality examples\n",
      "üìÅ Saved to: /content/autotrain_text.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "output_path = \"/content/autotrain_text.jsonl\"\n",
    "\n",
    "topics = [\n",
    "    \"Generative Artificial Intelligence\",\n",
    "    \"Large Language Models\",\n",
    "    \"Transformers\",\n",
    "    \"Self-Attention Mechanism\",\n",
    "    \"Fine-tuning\",\n",
    "    \"LoRA (Low-Rank Adaptation)\",\n",
    "    \"Parameter Efficient Fine-Tuning\",\n",
    "    \"Prompt Engineering\",\n",
    "    \"Instruction Tuning\",\n",
    "    \"Hallucinations in AI models\",\n",
    "    \"Synthetic Data\",\n",
    "    \"Machine Learning\",\n",
    "    \"Deep Learning\",\n",
    "    \"Neural Networks\",\n",
    "    \"Tokenization\",\n",
    "    \"Context Window\",\n",
    "    \"Embeddings\",\n",
    "    \"Vector Databases\",\n",
    "    \"Retrieval-Augmented Generation (RAG)\",\n",
    "    \"Overfitting\",\n",
    "    \"Generalization\",\n",
    "    \"Bias in AI systems\",\n",
    "    \"Evaluation Metrics in NLP\",\n",
    "]\n",
    "\n",
    "advanced_instruction_templates = [\n",
    "    \"Explain {topic} in simple terms.\",\n",
    "    \"Explain {topic} as if teaching a university student.\",\n",
    "    \"Explain {topic} using 5 bullet points.\",\n",
    "    \"Compare {topic} with traditional approaches.\",\n",
    "    \"Why is {topic} important in modern AI systems?\",\n",
    "    \"What are the advantages and disadvantages of {topic}?\",\n",
    "    \"Explain {topic} with a real-world example.\",\n",
    "    \"Describe the role of {topic} in large language models.\",\n",
    "    \"Explain {topic} step by step.\",\n",
    "    \"Explain {topic} and mention common mistakes beginners make.\",\n",
    "    \"How does {topic} improve model performance?\",\n",
    "    \"What would happen if {topic} is ignored in AI systems?\",\n",
    "    \"Explain {topic} in 3 clear paragraphs.\",\n",
    "    \"Summarize {topic} in exactly 3 sentences.\",\n",
    "    \"Explain {topic} in an academic style.\",\n",
    "    \"Explain {topic} for exam preparation purposes.\",\n",
    "    \"Discuss the relationship between {topic} and model generalization.\",\n",
    "    \"Analyze the impact of {topic} on AI reliability.\",\n",
    "    \"Give a detailed explanation of {topic} with examples.\",\n",
    "    \"Explain {topic} and compare it with a related concept.\",\n",
    "]\n",
    "\n",
    "response_templates = [\n",
    "    \"{topic} is a fundamental concept in artificial intelligence that plays a critical role in improving the performance and reliability of modern models. It helps systems understand complex patterns and generate meaningful outputs across various tasks.\",\n",
    "\n",
    "    \"In practical terms, {topic} allows AI systems to operate more efficiently and accurately. It is widely applied in areas such as natural language processing, recommendation systems, and intelligent assistants.\",\n",
    "\n",
    "    \"One of the main strengths of {topic} is its ability to enhance generalization. By applying this concept correctly, models become more robust and better suited for real-world deployment.\",\n",
    "\n",
    "    \"From an academic perspective, {topic} represents a key advancement in the field of artificial intelligence. Researchers continue to explore its potential to improve scalability, interpretability, and efficiency.\",\n",
    "\n",
    "    \"{topic} contributes significantly to model performance by optimizing how information is processed and learned. Without it, many modern AI systems would struggle to achieve high accuracy and reliability.\",\n",
    "\n",
    "    \"A real-world example of {topic} can be seen in applications like ChatGPT, autonomous systems, and advanced data analysis tools, where intelligent behavior depends heavily on this concept.\",\n",
    "\n",
    "    \"Understanding {topic} is essential for students and engineers working in AI, as it connects theoretical foundations with practical implementations in modern systems.\",\n",
    "]\n",
    "\n",
    "def generate_advanced_dataset(n=800):\n",
    "    dataset = []\n",
    "    for _ in range(n):\n",
    "        topic = random.choice(topics)\n",
    "        instruction = random.choice(advanced_instruction_templates).format(topic=topic)\n",
    "        response = random.choice(response_templates).format(topic=topic)\n",
    "\n",
    "        text = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\"\n",
    "        dataset.append({\"text\": text})\n",
    "    return dataset\n",
    "\n",
    "data = generate_advanced_dataset(800)\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Generated {len(data)} high-quality examples\")\n",
    "print(f\"üìÅ Saved to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
